{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb2b16b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef255841",
   "metadata": {},
   "source": [
    "# テキストデータの処理\n",
    "今まででてきた特徴量は，連続値とカテゴリデータである．  \n",
    "現実ではこの2つに加えてテキストデータが出てくる．  \n",
    "テキストデータの処理方法は，連続値ともカテゴリデータとも異なる．  \n",
    "\n",
    "テキストデータは文字列で表される．  \n",
    "しかし，文字列が全てテキストというわけではなく，カテゴリデータのこともある．  \n",
    "文字列データは以下4つの種類に分類できる．  \n",
    "- カテゴリデータ\n",
    "- カテゴリに分類できるものの自由に書かれている文字列\n",
    "- 構造化された文字列（名前や住所など）\n",
    "- テキストデータ\n",
    "\n",
    "カテゴリデータは4章で書いた通り処理する．  \n",
    "カテゴリに分類できる自由な文字列は，最初は自動で処理し，後半は手動で分類していく．  \n",
    "構造化された文字列は難しい（らしい）  \n",
    "テキストデータは以下の通りに処理する．  \n",
    "ただ，2024年現在ならDL使うのがいいんだろうけどね．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b5ee61",
   "metadata": {},
   "source": [
    "# Bag of Wordsによるテキスト表現\n",
    "機械学習では，BoWが広く用いられている．  \n",
    "BoWは単語の現れる回数を数えテキストを表現する方法である．  \n",
    "BoW表現するには以下3ステップがある．  \n",
    "1. トークン分割\n",
    "2. ボキャブラリ構築\n",
    "3. エンコード\n",
    "\n",
    "これは，個々の文書を単語ごとに分割し，全ての文書で出てくる単語を数え，個々の文書に対して単語が現れる回数を数えるというステップ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "236c8f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bards_words =[\"The fool doth think he is wise,\",\n",
    "              \"but the wise man knows himself to be a fool\"]\n",
    "\n",
    "vect = CountVectorizer()\n",
    "vect.fit(bards_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ade297cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "{'the': 9, 'fool': 3, 'doth': 2, 'think': 10, 'he': 4, 'is': 6, 'wise': 12, 'but': 1, 'man': 8, 'knows': 7, 'himself': 5, 'to': 11, 'be': 0}\n"
     ]
    }
   ],
   "source": [
    "# 出てきた単語数とその単語のID\n",
    "\n",
    "print(len(vect.vocabulary_))\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "555229af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# IDの単語がその文書で出てきた回数\n",
    "\n",
    "bag_of_words = vect.transform(bards_words)\n",
    "print(bag_of_words.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a81575",
   "metadata": {},
   "source": [
    "# 実際の学習\n",
    "実際の文書では，ある文書は全文書に出てきた単語の数の特徴量をもつことになる．  \n",
    "この特徴量に対して以下3つの処理を行う（データによっては行わないほうがいいかも）\n",
    "- 出現回数が少ない単語や，複数形や三単現などの変形した単語を消す\n",
    "- 多すぎる単語(前置詞など)も意味がないので消す\n",
    "- tf-idfなどを用いた重みづけ\n",
    "\n",
    "そして，その特徴量に対してロジスティック回帰を用いて学習する．\n",
    "\n",
    "### tf-idf\n",
    "tf-idfは全文書にはあまり出てこないのに，ある文書には頻繁に現れる単語に大きな重みを与える．  \n",
    "これは，他の文書にはでてこない単語はその文書の内容をよく表しているという発想．  \n",
    "$N$を全文書の数，$N_w$を単語$w$が現れる文書の数とする．  \n",
    "また，$tf$は文書$d$中に$w$が現れる回数である．  \n",
    "すると，以下の式で$tfidf$は求められる．  \n",
    "\n",
    "$tfidf(w, d) = tf(log(\\frac{N_w+1}{N+1})+1)$\n",
    "\n",
    "また，このtfidfはユークリッド長が1になるようにスケール変換をする．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ef19c7",
   "metadata": {},
   "source": [
    "# 1単語よりも大きい単位のBoW\n",
    "1単語のBoWだと，単語の順番の情報が失われる．  \n",
    "これはつまりnot goodなどの語彙が反映されないことを示している．  \n",
    "\n",
    "この問題を解決するために，トークンをn-gramで特徴量とする方法が提案されている．  \n",
    "単語（トークン）2つや3つをひとまとまりと考え，ひとまとまりを特徴量として使用する．  \n",
    "ひとまとまりにする数がnのときn-gramとよぶ．\n",
    "\n",
    "最小値から最大値を指定し，それらのn-gramでBoWを行う．  \n",
    "最小値は1で，最大値は5くらいが精度向上に役にたつ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16022d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 39\n",
      "Vocabulary:\n",
      "['be', 'be fool', 'but', 'but the', 'but the wise', 'doth', 'doth think', 'doth think he', 'fool', 'fool doth', 'fool doth think', 'he', 'he is', 'he is wise', 'himself', 'himself to', 'himself to be', 'is', 'is wise', 'knows', 'knows himself', 'knows himself to', 'man', 'man knows', 'man knows himself', 'the', 'the fool', 'the fool doth', 'the wise', 'the wise man', 'think', 'think he', 'think he is', 'to', 'to be', 'to be fool', 'wise', 'wise man', 'wise man knows']\n"
     ]
    }
   ],
   "source": [
    "bards_words =[\"The fool doth think he is wise,\",\n",
    "              \"but the wise man knows himself to be a fool\"]\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(1, 3)).fit(bards_words)\n",
    "print(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(cv.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc3560a",
   "metadata": {},
   "source": [
    "### Pipelineで使う方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c083a5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression())\n",
    "\n",
    "param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "              \"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (1, 3)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc34ce6",
   "metadata": {},
   "source": [
    "# トークン1つ1つの処理\n",
    "ひとつひとつのトークンの処理は，単数形・複数形や，現在形・過去形などを同一に処理したほうがいい．  \n",
    "処理方法は，語幹を使って表現する方法やルールベースで処理する方法がある．  \n",
    "これは性能がわずかによくなる程度だが，性能をぎりぎりまで良くしたい場合は有効である．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e7b6a8",
   "metadata": {},
   "source": [
    "# 文章のトピックとクラスタリング\n",
    "それぞれの文章が何をトピックとしているかを表す方法がある．  \n",
    "それはクラスタリングだったり，成分分析だったりする．  \n",
    "クラスタリングは文書をひとつのクラス(トピック)にわける方法で，成分分析は複数のトピックの組合せにわける．  \n",
    "\n",
    "クラスタリングや成分分析では一般的な語彙は除いたほうがいいとされる．  \n",
    "よって，頻出単語は削除することが多い．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c040f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
